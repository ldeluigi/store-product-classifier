{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProductDetection.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "zdlCQFyABGy3",
        "7fG1BAIjUP_7",
        "8c-J_rx58YtO",
        "jcB1_JdoNOBu",
        "pOZ-LOYf_Zdd",
        "0_v6BmKqAgDB",
        "tO0QS6Lu3gse",
        "KfJSLC7l5Kiy",
        "Y7VOlJPTe9Vy",
        "WQ5oRPUvfx0U",
        "p9f3xs_2TcYl",
        "i-pwFBHT55Fa",
        "qv6_8IXp5gpI",
        "246wwjyLC5lH",
        "ySBSaUa7C5lI",
        "5Q2zxxiAC5lJ",
        "ck5MYYWaC5lJ",
        "vidZuqvqC5lK",
        "2VMo_Mg3C5lO"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ldeluigi/supermarket-2077-product-vision/blob/master/ProductDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdlCQFyABGy3"
      },
      "source": [
        "## Download datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQmWhRKUE5oy"
      },
      "source": [
        "!rm -rf sample_data\r\n",
        "!gdown --id 1fDr4g4wbnSRkuCYyS3wpuJS7Ax22bVB_ -O all.zip\r\n",
        "!unzip -oq all.zip\r\n",
        "\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fG1BAIjUP_7"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLso_ssrUTuB"
      },
      "source": [
        "import scipy.io\r\n",
        "import os\r\n",
        "from pathlib import Path\r\n",
        "import re\r\n",
        "import cv2\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import math\r\n",
        "import itertools\r\n",
        "import shutil\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c-J_rx58YtO"
      },
      "source": [
        "## Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDIdYTwp8ep0"
      },
      "source": [
        "training_dirname = 'Training'\r\n",
        "\r\n",
        "def create_class_label(class_index, class_name):\r\n",
        "  return class_name\r\n",
        "\r\n",
        "def read_classes():\r\n",
        "  mat = scipy.io.loadmat(os.path.join(training_dirname, 'TrainingClassesIndex.mat'))\r\n",
        "  raw_classes = list(map(lambda x: x[0], mat['classes'][0]))\r\n",
        "  classes = map(lambda x: (x[0], create_class_label(*x)), enumerate(raw_classes, start=1))\r\n",
        "  return dict(classes), dict(enumerate(raw_classes, start=1))\r\n",
        "\r\n",
        "def read_training_data(classes):\r\n",
        "  result = []\r\n",
        "  for class_index, class_name in classes.items():\r\n",
        "    dirname_images = os.path.join(training_dirname, class_name)\r\n",
        "    directory_images = os.fsencode(dirname_images)\r\n",
        "    for file in os.listdir(directory_images):\r\n",
        "      img = cv2.imread(os.path.join(dirname_images, os.fsdecode(file)))\r\n",
        "      img_rgb =  cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n",
        "      result.append((img_rgb, class_index))\r\n",
        "  return np.rec.array(result, dtype=[('image', 'O'), ('class_index', 'i4')])\r\n",
        "\r\n",
        "def read_store_data(storename):\r\n",
        "  dirname_anno = os.path.join(storename, 'annotation')\r\n",
        "  dirname_images = os.path.join(storename, 'images')\r\n",
        "  directory_anno = os.fsencode(dirname_anno)\r\n",
        "  directory_images = os.fsencode(dirname_images)\r\n",
        "\r\n",
        "  result = []\r\n",
        "\r\n",
        "  for file in os.listdir(directory_anno):\r\n",
        "    filename = os.fsdecode(file)\r\n",
        "    if filename.endswith(\".mat\"): \r\n",
        "      mat = scipy.io.loadmat(os.path.join(dirname_anno, filename))\r\n",
        "      number = re.search(r'^anno.(\\d+).mat$', filename).group(1)\r\n",
        "      img = cv2.imread(os.path.join(dirname_images, number + '.jpg'))\r\n",
        "\r\n",
        "      img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n",
        "      img_annotation = mat['annotation'][0, 0]\r\n",
        "      bboxes = map(lambda x: x[0], img_annotation[0][0])\r\n",
        "      labels = map(lambda x: str(x[0][0][0]), img_annotation[1][0])\r\n",
        "      class_indexes = img_annotation[2][0]\r\n",
        "      result.append((img_rgb, list(zip(bboxes, labels, class_indexes))))\r\n",
        "  return np.rec.array(result, dtype=[('image', 'O'), ('items', 'O')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcB1_JdoNOBu"
      },
      "source": [
        "## Data visualization utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x36OMLMbXC4"
      },
      "source": [
        "def show_image(img):\r\n",
        "  plt.axis('off')\r\n",
        "  plt.imshow(img)\r\n",
        "\r\n",
        "def show_grayscale_image(img):\r\n",
        "  show_image(cv2.merge([img, img, img]))\r\n",
        "\r\n",
        "def plot_grid(images, columns, show_axis=False):\r\n",
        "  height = 1 + math.ceil(len(images) / columns) * 2\r\n",
        "  width = columns * 2\r\n",
        "  dpi = max(images[0].shape[0], images[0].shape[1]) // 4\r\n",
        "  fig = plt.figure(figsize=(width, height), dpi=dpi)\r\n",
        "  fig.subplots_adjust(hspace=0.4)\r\n",
        "  for index, img in enumerate(images, start=1):\r\n",
        "    if 'float' in img.dtype.str:\r\n",
        "      img = (img * 255).astype('uint8')\r\n",
        "    sp = fig.add_subplot(math.ceil(len(images) / columns), columns, index)\r\n",
        "    if not show_axis:\r\n",
        "      plt.axis('off')\r\n",
        "    plt.imshow(img)\r\n",
        "    sp.set_title(index, fontsize=10)\r\n",
        "\r\n",
        "def dataset_plot_grid(indexes, columns, dataset, draw_item):\r\n",
        "  fig = plt.figure(figsize=(12, 6), dpi=120)\r\n",
        "  # fig.subplots_adjust(hspace=0.2)\r\n",
        "  for index, i_img in enumerate(indexes, start=1):\r\n",
        "    sp = fig.add_subplot(math.ceil(len(indexes) / columns), columns, index)\r\n",
        "    row = dataset[i_img]\r\n",
        "    draw_item(row, sp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOZ-LOYf_Zdd"
      },
      "source": [
        "# Product Class Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_v6BmKqAgDB"
      },
      "source": [
        "## Prepare products class dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDqS6CthBF6Z"
      },
      "source": [
        "classes, raw_classes = read_classes()\r\n",
        "\r\n",
        "def class_name(class_index):\r\n",
        "  return classes[class_index] if class_index >= 0 else None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO0QS6Lu3gse"
      },
      "source": [
        "## Load training raw images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4TJnjzL0XsC"
      },
      "source": [
        "products = read_training_data(raw_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfJSLC7l5Kiy"
      },
      "source": [
        "## Products visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bjFqgExNdla"
      },
      "source": [
        "def show_products_with_class(indexes, columns, dataset):\r\n",
        "  def show_single_product_with_class(row, sp):\r\n",
        "    plt.axis('off')\r\n",
        "    plt.imshow(row.image)\r\n",
        "    sp.set_title(class_name(row.class_index), fontsize=10)\r\n",
        "  dataset_plot_grid(indexes, columns, dataset, show_single_product_with_class)\r\n",
        "\r\n",
        "show_products_with_class(np.random.randint(0, len(products), 6), 3, products)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7VOlJPTe9Vy"
      },
      "source": [
        "## Image preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ5oRPUvfx0U"
      },
      "source": [
        "### Background removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Dzhh2G0fs-e"
      },
      "source": [
        "# code taken from https://www.kaggle.com/vadbeg/opencv-background-removal and modified\r\n",
        "\r\n",
        "def remove_background(img, threshold):\r\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\r\n",
        "    _, threshed = cv2.threshold(gray, threshold, 255, cv2.THRESH_BINARY_INV)\r\n",
        "\r\n",
        "    kernel_size = round(max(img.shape[0], img.shape[1]) * 0.02)\r\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\r\n",
        "    morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)\r\n",
        "\r\n",
        "    cnts = cv2.findContours(morphed, \r\n",
        "                            cv2.RETR_EXTERNAL,\r\n",
        "                            cv2.CHAIN_APPROX_SIMPLE)[0]\r\n",
        "\r\n",
        "    cnts = sorted(cnts, key=cv2.contourArea)\r\n",
        "\r\n",
        "    mask = cv2.drawContours(threshed, [cnts[-1]], 0, [255], cv2.FILLED)\r\n",
        "    masked_data = cv2.bitwise_and(img, img, mask=mask)\r\n",
        "\r\n",
        "    x, y, w, h = cv2.boundingRect(cnts[-1])\r\n",
        "    dst = masked_data[y: y + h, x: x + w]\r\n",
        "\r\n",
        "    alpha = mask[y: y + h, x: x + w]\r\n",
        "    r, g, b = cv2.split(dst)\r\n",
        "\r\n",
        "    rgba = [r, g, b, alpha]\r\n",
        "    dst = cv2.merge(rgba, 4)\r\n",
        "    return dst\r\n",
        "\r\n",
        "n = np.random.randint(products.shape[0])\r\n",
        "print(f'Index: {n}')\r\n",
        "print(f'Class: {class_name(products[n].class_index)}')\r\n",
        "plot_grid([products[n].image, remove_background(products[n].image, 250)], 2, show_axis=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9f3xs_2TcYl"
      },
      "source": [
        "### Image resize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_c8Rek4T49O"
      },
      "source": [
        "def resize_image(img, size, color=[0,0,0,0]):\r\n",
        "  target_w, target_h = size\r\n",
        "  original_h, original_w, _ = img.shape\r\n",
        "  target_ar = target_w / target_h\r\n",
        "  original_ar = original_w / original_h\r\n",
        "\r\n",
        "  scale_factor = target_h / original_h if target_ar > original_ar else target_w / original_w\r\n",
        "  scaled_w = round(original_w * scale_factor)\r\n",
        "  scaled_h = round(original_h * scale_factor)\r\n",
        "  scaled_size = (scaled_w, scaled_h)\r\n",
        "  resized = cv2.resize(img, scaled_size)\r\n",
        "\r\n",
        "  delta_h = target_h - scaled_h\r\n",
        "  delta_w = target_w - scaled_w\r\n",
        "  top    = delta_h // 2\r\n",
        "  left   = delta_w // 2\r\n",
        "  bottom = delta_h - top\r\n",
        "  right  = delta_w - left\r\n",
        "\r\n",
        "  return cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\r\n",
        "\r\n",
        "n = np.random.randint(products.shape[0])\r\n",
        "image = products[n].image\r\n",
        "image = remove_background(image, 250)\r\n",
        "plot_grid([image, resize_image(image, (400, 400))], 2, show_axis=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLKEGiR1ozQi"
      },
      "source": [
        "## Dataset preparation\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-pwFBHT55Fa"
      },
      "source": [
        "### Image cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUuyRSE0t4GQ"
      },
      "source": [
        "size = (224, 224)\r\n",
        "\r\n",
        "def clean_image(img):\r\n",
        "  threshold = 250\r\n",
        "  img = remove_background(img, threshold)\r\n",
        "  return resize_image(img, size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXGPRPzVuJKX"
      },
      "source": [
        "### Export new dataset on file\r\n",
        "\r\n",
        "To save memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apCSNhg43YNC"
      },
      "source": [
        "number_of_images_to_save = 6400\r\n",
        "reduce_dataset = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79Ba0_JdG6_X"
      },
      "source": [
        "def get_class_from_sub_classes(complete_class):\r\n",
        "  sub_classes = complete_class.split('/')\r\n",
        "  top_level_class = sub_classes[0]\r\n",
        "  if top_level_class == 'Food' or top_level_class == 'HouseProducts':\r\n",
        "    return sub_classes[1]\r\n",
        "  else:\r\n",
        "    return top_level_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNE4hWdsoylg"
      },
      "source": [
        "train_data_directory = 'Temp'\r\n",
        "shutil.rmtree(train_data_directory, ignore_errors=True)\r\n",
        "\r\n",
        "if reduce_dataset:\r\n",
        "  products_size = number_of_images_to_save\r\n",
        "  products_to_dump = np.random.choice(products.shape[0], number_of_images_to_save, replace = False)\r\n",
        "else:\r\n",
        "  products_size = products.shape[0]\r\n",
        "  products_to_dump = np.arange(products_size)\r\n",
        "\r\n",
        "for index, (image, class_index) in tqdm(enumerate(products[products_to_dump]), total=products_size, desc='Writing files...'):\r\n",
        "  complete_class = class_name(class_index)\r\n",
        "  class_directory = get_class_from_sub_classes(complete_class)\r\n",
        "  output_dir = os.path.join(train_data_directory, class_directory)\r\n",
        "  Path(output_dir).mkdir(parents=True, exist_ok=True)\r\n",
        "  out = cv2.cvtColor(clean_image(image), cv2.COLOR_RGBA2BGRA)\r\n",
        "  cv2.imwrite(os.path.join(output_dir, f'{class_directory}.{index}.png'), out)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANnhDb-si674"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv6_8IXp5gpI"
      },
      "source": [
        "### 3D rotation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7ysgkyopSmx"
      },
      "source": [
        "def image_3D_rotation(img, theta = 0, phi = 0, gamma = 0, dx = 0, dy = 0, dz = 0):\r\n",
        "  \"\"\"\r\n",
        "  Parameters:\r\n",
        "      img       : the image data as numpy array\r\n",
        "      theta     : rotation around the x axis\r\n",
        "      phi       : rotation around the y axis\r\n",
        "      gamma     : rotation around the z axis (basically a 2D rotation)\r\n",
        "      dx        : translation along the x axis\r\n",
        "      dy        : translation along the y axis\r\n",
        "      dz        : translation along the z axis (distance to the image)\r\n",
        "  Output:\r\n",
        "      image     : the rotated image\r\n",
        "  \r\n",
        "  Reference:\r\n",
        "      1.        : http://stackoverflow.com/questions/17087446/how-to-calculate-perspective-transform-for-opencv-from-rotation-angles\r\n",
        "      2.        : http://jepsonsblog.blogspot.tw/2012/11/rotation-in-3d-using-opencvs.html\r\n",
        "      3.        : Code taken from https://github.com/eborboihuc/rotate_3d/blob/master/image_transformer.py\r\n",
        "  \"\"\"\r\n",
        "  def deg_to_rad(deg):\r\n",
        "    return deg * math.pi / 180.0\r\n",
        "  def get_M(theta, phi, gamma, dx, dy, dz, size, focal):\r\n",
        "    w = size[0]\r\n",
        "    h = size[1]\r\n",
        "    f = focal\r\n",
        "    # Projection 2D -> 3D matrix\r\n",
        "    A1 = np.array([ [1, 0, -w/2],\r\n",
        "                    [0, 1, -h/2],\r\n",
        "                    [0, 0, 1],\r\n",
        "                    [0, 0, 1]])\r\n",
        "    # Rotation matrices around the X, Y, and Z axis\r\n",
        "    RX = np.array([ [1, 0, 0, 0],\r\n",
        "                    [0, np.cos(theta), -np.sin(theta), 0],\r\n",
        "                    [0, np.sin(theta), np.cos(theta), 0],\r\n",
        "                    [0, 0, 0, 1]])\r\n",
        "    RY = np.array([ [np.cos(phi), 0, -np.sin(phi), 0],\r\n",
        "                    [0, 1, 0, 0],\r\n",
        "                    [np.sin(phi), 0, np.cos(phi), 0],\r\n",
        "                    [0, 0, 0, 1]])\r\n",
        "    RZ = np.array([ [np.cos(gamma), -np.sin(gamma), 0, 0],\r\n",
        "                    [np.sin(gamma), np.cos(gamma), 0, 0],\r\n",
        "                    [0, 0, 1, 0],\r\n",
        "                    [0, 0, 0, 1]])\r\n",
        "    # Composed rotation matrix with (RX, RY, RZ)\r\n",
        "    R = np.dot(np.dot(RX, RY), RZ)\r\n",
        "    # Translation matrix\r\n",
        "    T = np.array([  [1, 0, 0, dx],\r\n",
        "                    [0, 1, 0, dy],\r\n",
        "                    [0, 0, 1, dz],\r\n",
        "                    [0, 0, 0, 1]])\r\n",
        "    # Projection 3D -> 2D matrix\r\n",
        "    A2 = np.array([ [f, 0, w/2, 0],\r\n",
        "                    [0, f, h/2, 0],\r\n",
        "                    [0, 0, 1, 0]])\r\n",
        "    # Final transformation matrix\r\n",
        "    return np.dot(A2, np.dot(T, np.dot(R, A1)))\r\n",
        "  height = img.shape[0]\r\n",
        "  width = img.shape[1]\r\n",
        "  num_channels = img.shape[2]\r\n",
        "  rtheta = deg_to_rad(theta)\r\n",
        "  rphi = deg_to_rad(phi)\r\n",
        "  rgamma = deg_to_rad(gamma)\r\n",
        "  d = np.sqrt(height**2 + width**2)\r\n",
        "  focal = d / (2 * np.sin(rgamma) if np.sin(rgamma) != 0 else 1)\r\n",
        "  dz = focal\r\n",
        "  mat = get_M(rtheta, rphi, rgamma, dx, dy, dz, (width, height), focal)\r\n",
        "  return cv2.warpPerspective(img.copy(), mat, (width, height))\r\n",
        "\r\n",
        "def random_spatial_rotation(theta_range, phi_range, gamma_range):\r\n",
        "  return lambda img: image_3D_rotation(\r\n",
        "    img, \r\n",
        "    theta = np.random.randint(theta_range[0], theta_range[1] + 1),\r\n",
        "    phi = np.random.randint(phi_range[0], phi_range[1] + 1),\r\n",
        "    gamma = np.random.randint(gamma_range[0], gamma_range[1] + 1)\r\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2LXXq6w5oEV"
      },
      "source": [
        "### Data generator parameters definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1e09fgCa9-T"
      },
      "source": [
        "datagen_kwargs = dict(\r\n",
        "  brightness_range = [0.5, 1.2],\r\n",
        "  width_shift_range = size[0] // 8,\r\n",
        "  height_shift_range = size[1] // 8,\r\n",
        "  zoom_range = 0.1,\r\n",
        "  fill_mode = 'constant',\r\n",
        "  cval = 0,\r\n",
        "  data_format = 'channels_last',\r\n",
        "  preprocessing_function = random_spatial_rotation(\r\n",
        "        theta_range = (-20, 20),\r\n",
        "        phi_range = (-30, 30),\r\n",
        "        gamma_range = (-10, 10)\r\n",
        "      ),\r\n",
        "  rescale = 1.0 / 255,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_bLFqPm6Bzz"
      },
      "source": [
        "## [1] Training with splitting and augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZCh6txx5wqv"
      },
      "source": [
        "### Data flow definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNoE6DPOj2mA"
      },
      "source": [
        "datagen = ImageDataGenerator(\r\n",
        "  **datagen_kwargs,\r\n",
        "  validation_split = 0.2\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEVrYzc7A1Ni"
      },
      "source": [
        "demo_flow = datagen.flow_from_directory(\r\n",
        "  directory = train_data_directory,\r\n",
        "  target_size = size,\r\n",
        "  color_mode = 'rgba',\r\n",
        "  class_mode = 'categorical',\r\n",
        "  batch_size = 1,\r\n",
        "  shuffle = True\r\n",
        ")\r\n",
        "plot_grid([next(demo_flow)[0][0] for _ in range(40)], 4)\r\n",
        "del demo_flow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEIlo_Y7F2ve"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cn2fXra6F2Am"
      },
      "source": [
        "alpha = 1.0\r\n",
        "weights = 'imagenet'\r\n",
        "pooling = 'max'\r\n",
        "activation = 'softmax'\r\n",
        "batch_size = 32\r\n",
        "epochs = 20\r\n",
        "optimizer = 'rmsprop'\r\n",
        "loss = 'categorical_crossentropy'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e502eZ22Lm0p"
      },
      "source": [
        "### Generator definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOzE92E2Lmdv"
      },
      "source": [
        "def create_flow(subset):\r\n",
        "  return datagen.flow_from_directory(\r\n",
        "    directory = train_data_directory,\r\n",
        "    target_size = size,\r\n",
        "    color_mode = 'rgb',\r\n",
        "    class_mode = 'categorical',\r\n",
        "    batch_size = batch_size,\r\n",
        "    shuffle = True,\r\n",
        "    subset=subset\r\n",
        "  )\r\n",
        "\r\n",
        "train_flow = create_flow('training')\r\n",
        "\r\n",
        "validation_flow = create_flow('validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4kpRup6F9N-"
      },
      "source": [
        "### Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2KSEQBh6FAd"
      },
      "source": [
        "model = tf.keras.applications.MobileNetV2(\r\n",
        "  input_shape = (*size, 3),\r\n",
        "  alpha = alpha,\r\n",
        "  include_top = False,\r\n",
        "  weights = weights,\r\n",
        "  pooling = pooling,\r\n",
        "  classes = train_flow.num_classes,\r\n",
        "  classifier_activation = activation\r\n",
        ")\r\n",
        "\r\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\r\n",
        "#tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf-3JwRRLFsK"
      },
      "source": [
        "### Training phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1iJIwJBLKvb"
      },
      "source": [
        "history = model.fit(\r\n",
        "  train_flow,\r\n",
        "  validation_data = validation_flow,\r\n",
        "  validation_steps = validation_flow.samples // batch_size,\r\n",
        "  epochs = epochs,\r\n",
        "  steps_per_epoch = train_flow.samples // batch_size\r\n",
        ")\r\n",
        "\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('Model accuracy')\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJ1wJmnQb_8u"
      },
      "source": [
        "## [2] Training with validation generated from same set as train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRcgTyINb_81"
      },
      "source": [
        "### Data flow definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlZHe6tab_81"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\r\n",
        "  **datagen_kwargs\r\n",
        ")\r\n",
        "test_datagen = ImageDataGenerator(\r\n",
        "    rescale = 1./255\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5kSStN9b_82"
      },
      "source": [
        "demo_flow = datagen.flow_from_directory(\r\n",
        "  directory = train_data_directory,\r\n",
        "  target_size = size,\r\n",
        "  color_mode = 'rgba',\r\n",
        "  class_mode = 'categorical',\r\n",
        "  batch_size = 1,\r\n",
        "  shuffle = True\r\n",
        ")\r\n",
        "plot_grid([next(demo_flow)[0][0] for _ in range(40)], 4)\r\n",
        "del demo_flow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK23S8gUb_82"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JRN07bVb_83"
      },
      "source": [
        "alpha = 1.0\r\n",
        "weights = None\r\n",
        "pooling = 'max'\r\n",
        "activation = 'softmax'\r\n",
        "batch_size = 1\r\n",
        "epochs = 20\r\n",
        "optimizer = 'adam'\r\n",
        "loss = 'categorical_crossentropy'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m_bQKeJb_82"
      },
      "source": [
        "### Generator definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJYSo7GWb_82"
      },
      "source": [
        "def create_flow(datagen):\r\n",
        "  return datagen.flow_from_directory(\r\n",
        "    directory = train_data_directory,\r\n",
        "    target_size = size,\r\n",
        "    color_mode = 'rgb',\r\n",
        "    class_mode = 'categorical',\r\n",
        "    batch_size = batch_size,\r\n",
        "    shuffle = True\r\n",
        "  )\r\n",
        "\r\n",
        "train_flow = create_flow(train_datagen)\r\n",
        "validation_flow = create_flow(test_datagen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbrFhZw3b_83"
      },
      "source": [
        "### Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi-9WYqOb_83"
      },
      "source": [
        "model = tf.keras.applications.MobileNetV2(\r\n",
        "  input_shape = (*size, 3),\r\n",
        "  alpha = alpha,\r\n",
        "  include_top = True,\r\n",
        "  pooling = pooling,\r\n",
        "  weights = weights,\r\n",
        "  classes = train_flow.num_classes,\r\n",
        "  classifier_activation = activation\r\n",
        ")\r\n",
        "\r\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\r\n",
        "#tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsMGNZplb_83"
      },
      "source": [
        "### Training phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CrDtPX4b_83"
      },
      "source": [
        "history = model.fit(\r\n",
        "  train_flow,\r\n",
        "  validation_data = validation_flow,\r\n",
        "  validation_steps = validation_flow.samples // batch_size,\r\n",
        "  epochs = epochs,\r\n",
        "  steps_per_epoch = train_flow.samples // batch_size\r\n",
        ")\r\n",
        "\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('Model accuracy')\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw68u1pt31-X"
      },
      "source": [
        "## Store images visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-OrlAhPYOH2"
      },
      "source": [
        "def show_shelf_with_bbox_and_classes(indexes, columns, dataset):\r\n",
        "  def show_single_shelf(row, sp):\r\n",
        "    color = (255, 255, 0)\r\n",
        "    img = row.image.copy()\r\n",
        "    for item in row.items:\r\n",
        "      bbox, label, class_index  = item\r\n",
        "      [xmin, xmax, ymin, ymax] = bbox\r\n",
        "      height, width, _ = img.shape\r\n",
        "      xmin = int(xmin * width)\r\n",
        "      xmax = int(xmax * width)\r\n",
        "      ymin = int(ymin * height)\r\n",
        "      ymax = int(ymax * height)\r\n",
        "      thickness = round(max(width, height) * 0.01)\r\n",
        "      cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color, thickness)\r\n",
        "      cv2.putText(img,\r\n",
        "                  class_name(class_index),\r\n",
        "                  (round(xmin + thickness * 1.5), round(ymin + thickness * 3)),\r\n",
        "                  cv2.FONT_HERSHEY_SIMPLEX,\r\n",
        "                  2.5,\r\n",
        "                  color,\r\n",
        "                  thickness=round(thickness/2))\r\n",
        "    plt.axis('off')\r\n",
        "    plt.imshow(img)\r\n",
        "  dataset_plot_grid(indexes, columns, dataset, show_single_shelf)\r\n",
        "\r\n",
        "show_shelf_with_bbox_and_classes(np.random.randint(0, len(store), 4), 2, store)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dkAjvNEC5lE"
      },
      "source": [
        "# Specific Product Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "246wwjyLC5lH"
      },
      "source": [
        "## Load training raw images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_sD-0L5C5lH"
      },
      "source": [
        "products = read_training_data(raw_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySBSaUa7C5lI"
      },
      "source": [
        "## Image preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDvtnGseC5lI"
      },
      "source": [
        "### Background removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiCZMlzfC5lI"
      },
      "source": [
        "# code taken from https://www.kaggle.com/vadbeg/opencv-background-removal and modified\r\n",
        "\r\n",
        "def remove_background(img, threshold):\r\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\r\n",
        "    _, threshed = cv2.threshold(gray, threshold, 255, cv2.THRESH_BINARY_INV)\r\n",
        "\r\n",
        "    kernel_size = round(max(img.shape[0], img.shape[1]) * 0.02)\r\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\r\n",
        "    morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)\r\n",
        "\r\n",
        "    cnts = cv2.findContours(morphed, \r\n",
        "                            cv2.RETR_EXTERNAL,\r\n",
        "                            cv2.CHAIN_APPROX_SIMPLE)[0]\r\n",
        "\r\n",
        "    cnts = sorted(cnts, key=cv2.contourArea)\r\n",
        "\r\n",
        "    mask = cv2.drawContours(threshed, [cnts[-1]], 0, [255], cv2.FILLED)\r\n",
        "    masked_data = cv2.bitwise_and(img, img, mask=mask)\r\n",
        "\r\n",
        "    x, y, w, h = cv2.boundingRect(cnts[-1])\r\n",
        "    dst = masked_data[y: y + h, x: x + w]\r\n",
        "\r\n",
        "    alpha = mask[y: y + h, x: x + w]\r\n",
        "    r, g, b = cv2.split(dst)\r\n",
        "\r\n",
        "    rgba = [r, g, b, alpha]\r\n",
        "    dst = cv2.merge(rgba, 4)\r\n",
        "    return dst\r\n",
        "\r\n",
        "n = np.random.randint(products.shape[0])\r\n",
        "print(f'Index: {n}')\r\n",
        "print(f'Class: {class_name(products[n].class_index)}')\r\n",
        "plot_grid([products[n].image, remove_background(products[n].image, 250)], 2, show_axis=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH1-IQ7fC5lJ"
      },
      "source": [
        "### Image resize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBsfa18aC5lJ"
      },
      "source": [
        "def resize_image(img, size, color=[0,0,0,0]):\r\n",
        "  target_w, target_h = size\r\n",
        "  original_h, original_w, _ = img.shape\r\n",
        "  target_ar = target_w / target_h\r\n",
        "  original_ar = original_w / original_h\r\n",
        "\r\n",
        "  scale_factor = target_h / original_h if target_ar > original_ar else target_w / original_w\r\n",
        "  scaled_w = round(original_w * scale_factor)\r\n",
        "  scaled_h = round(original_h * scale_factor)\r\n",
        "  scaled_size = (scaled_w, scaled_h)\r\n",
        "  resized = cv2.resize(img, scaled_size)\r\n",
        "\r\n",
        "  delta_h = target_h - scaled_h\r\n",
        "  delta_w = target_w - scaled_w\r\n",
        "  top    = delta_h // 2\r\n",
        "  left   = delta_w // 2\r\n",
        "  bottom = delta_h - top\r\n",
        "  right  = delta_w - left\r\n",
        "\r\n",
        "  return cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\r\n",
        "\r\n",
        "n = np.random.randint(products.shape[0])\r\n",
        "image = products[n].image\r\n",
        "image = remove_background(image, 250)\r\n",
        "plot_grid([image, resize_image(image, (400, 400))], 2, show_axis=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm91uvypC5lJ"
      },
      "source": [
        "## Dataset preparation\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q2zxxiAC5lJ"
      },
      "source": [
        "### Image cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O67s6wUvC5lJ"
      },
      "source": [
        "size = (224, 224)\r\n",
        "\r\n",
        "def clean_image(img):\r\n",
        "  threshold = 250\r\n",
        "  img = remove_background(img, threshold)\r\n",
        "  return resize_image(img, size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck5MYYWaC5lJ"
      },
      "source": [
        "### Export new dataset on file\r\n",
        "\r\n",
        "To save memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll-IG7RAC5lJ"
      },
      "source": [
        "number_of_images_to_save = 6400\r\n",
        "reduce_dataset = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1omOsG5-C5lK"
      },
      "source": [
        "train_data_directory = 'Temp'\r\n",
        "shutil.rmtree(train_data_directory, ignore_errors=True)\r\n",
        "\r\n",
        "if reduce_dataset:\r\n",
        "  products_size = number_of_images_to_save\r\n",
        "  products_to_dump = np.random.choice(products.shape[0], number_of_images_to_save, replace = False)\r\n",
        "else:\r\n",
        "  products_size = products.shape[0]\r\n",
        "  products_to_dump = np.arange(products_size)\r\n",
        "\r\n",
        "for index, (image, class_index) in tqdm(enumerate(products[products_to_dump]), total=products_size, desc='Writing files...'):\r\n",
        "  class_directory = str(index)\r\n",
        "  output_dir = os.path.join(train_data_directory, class_directory)\r\n",
        "  Path(output_dir).mkdir(parents=True, exist_ok=True)\r\n",
        "  out = cv2.cvtColor(clean_image(image), cv2.COLOR_RGBA2BGRA)\r\n",
        "  cv2.imwrite(os.path.join(output_dir, f'{class_directory}.{index}.png'), out)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFXlY-1sC5lK"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vidZuqvqC5lK"
      },
      "source": [
        "### 3D rotation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA76l9AHC5lK"
      },
      "source": [
        "def image_3D_rotation(img, theta = 0, phi = 0, gamma = 0, dx = 0, dy = 0, dz = 0):\r\n",
        "  \"\"\"\r\n",
        "  Parameters:\r\n",
        "      img       : the image data as numpy array\r\n",
        "      theta     : rotation around the x axis\r\n",
        "      phi       : rotation around the y axis\r\n",
        "      gamma     : rotation around the z axis (basically a 2D rotation)\r\n",
        "      dx        : translation along the x axis\r\n",
        "      dy        : translation along the y axis\r\n",
        "      dz        : translation along the z axis (distance to the image)\r\n",
        "  Output:\r\n",
        "      image     : the rotated image\r\n",
        "  \r\n",
        "  Reference:\r\n",
        "      1.        : http://stackoverflow.com/questions/17087446/how-to-calculate-perspective-transform-for-opencv-from-rotation-angles\r\n",
        "      2.        : http://jepsonsblog.blogspot.tw/2012/11/rotation-in-3d-using-opencvs.html\r\n",
        "      3.        : Code taken from https://github.com/eborboihuc/rotate_3d/blob/master/image_transformer.py\r\n",
        "  \"\"\"\r\n",
        "  def deg_to_rad(deg):\r\n",
        "    return deg * math.pi / 180.0\r\n",
        "  def get_M(theta, phi, gamma, dx, dy, dz, size, focal):\r\n",
        "    w = size[0]\r\n",
        "    h = size[1]\r\n",
        "    f = focal\r\n",
        "    # Projection 2D -> 3D matrix\r\n",
        "    A1 = np.array([ [1, 0, -w/2],\r\n",
        "                    [0, 1, -h/2],\r\n",
        "                    [0, 0, 1],\r\n",
        "                    [0, 0, 1]])\r\n",
        "    # Rotation matrices around the X, Y, and Z axis\r\n",
        "    RX = np.array([ [1, 0, 0, 0],\r\n",
        "                    [0, np.cos(theta), -np.sin(theta), 0],\r\n",
        "                    [0, np.sin(theta), np.cos(theta), 0],\r\n",
        "                    [0, 0, 0, 1]])\r\n",
        "    RY = np.array([ [np.cos(phi), 0, -np.sin(phi), 0],\r\n",
        "                    [0, 1, 0, 0],\r\n",
        "                    [np.sin(phi), 0, np.cos(phi), 0],\r\n",
        "                    [0, 0, 0, 1]])\r\n",
        "    RZ = np.array([ [np.cos(gamma), -np.sin(gamma), 0, 0],\r\n",
        "                    [np.sin(gamma), np.cos(gamma), 0, 0],\r\n",
        "                    [0, 0, 1, 0],\r\n",
        "                    [0, 0, 0, 1]])\r\n",
        "    # Composed rotation matrix with (RX, RY, RZ)\r\n",
        "    R = np.dot(np.dot(RX, RY), RZ)\r\n",
        "    # Translation matrix\r\n",
        "    T = np.array([  [1, 0, 0, dx],\r\n",
        "                    [0, 1, 0, dy],\r\n",
        "                    [0, 0, 1, dz],\r\n",
        "                    [0, 0, 0, 1]])\r\n",
        "    # Projection 3D -> 2D matrix\r\n",
        "    A2 = np.array([ [f, 0, w/2, 0],\r\n",
        "                    [0, f, h/2, 0],\r\n",
        "                    [0, 0, 1, 0]])\r\n",
        "    # Final transformation matrix\r\n",
        "    return np.dot(A2, np.dot(T, np.dot(R, A1)))\r\n",
        "  height = img.shape[0]\r\n",
        "  width = img.shape[1]\r\n",
        "  num_channels = img.shape[2]\r\n",
        "  rtheta = deg_to_rad(theta)\r\n",
        "  rphi = deg_to_rad(phi)\r\n",
        "  rgamma = deg_to_rad(gamma)\r\n",
        "  d = np.sqrt(height**2 + width**2)\r\n",
        "  focal = d / (2 * np.sin(rgamma) if np.sin(rgamma) != 0 else 1)\r\n",
        "  dz = focal\r\n",
        "  mat = get_M(rtheta, rphi, rgamma, dx, dy, dz, (width, height), focal)\r\n",
        "  return cv2.warpPerspective(img.copy(), mat, (width, height))\r\n",
        "\r\n",
        "def random_spatial_rotation(theta_range, phi_range, gamma_range):\r\n",
        "  return lambda img: image_3D_rotation(\r\n",
        "    img, \r\n",
        "    theta = np.random.randint(theta_range[0], theta_range[1] + 1),\r\n",
        "    phi = np.random.randint(phi_range[0], phi_range[1] + 1),\r\n",
        "    gamma = np.random.randint(gamma_range[0], gamma_range[1] + 1)\r\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VMo_Mg3C5lO"
      },
      "source": [
        "### Data generator parameters definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry8J1oM7C5lQ"
      },
      "source": [
        "datagen_kwargs = dict(\r\n",
        "  brightness_range = [0.5, 1.2],\r\n",
        "  width_shift_range = size[0] // 8,\r\n",
        "  height_shift_range = size[1] // 8,\r\n",
        "  zoom_range = 0.1,\r\n",
        "  fill_mode = 'constant',\r\n",
        "  cval = 0,\r\n",
        "  data_format = 'channels_last',\r\n",
        "  preprocessing_function = random_spatial_rotation(\r\n",
        "        theta_range = (-20, 20),\r\n",
        "        phi_range = (-30, 30),\r\n",
        "        gamma_range = (-10, 10)\r\n",
        "      ),\r\n",
        "  rescale = 1.0 / 255,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nI2IW4ZC5lR"
      },
      "source": [
        "## [2] Training with validation generated from same set as train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED-4WJDKC5lR"
      },
      "source": [
        "### Data flow definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKQtF-IuC5lS"
      },
      "source": [
        "train_datagen = ImageDataGenerator(\r\n",
        "  **datagen_kwargs\r\n",
        ")\r\n",
        "test_datagen = ImageDataGenerator(\r\n",
        "    rescale = 1./255\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0fIGxdUC5lS"
      },
      "source": [
        "demo_flow = datagen.flow_from_directory(\r\n",
        "  directory = train_data_directory,\r\n",
        "  target_size = size,\r\n",
        "  color_mode = 'rgba',\r\n",
        "  class_mode = 'categorical',\r\n",
        "  batch_size = 1,\r\n",
        "  shuffle = True\r\n",
        ")\r\n",
        "plot_grid([next(demo_flow)[0][0] for _ in range(40)], 4)\r\n",
        "del demo_flow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy1kxFViC5lS"
      },
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97rZi4DMC5lS"
      },
      "source": [
        "alpha = 1.0\r\n",
        "weights = None\r\n",
        "pooling = 'max'\r\n",
        "activation = 'softmax'\r\n",
        "batch_size = 1\r\n",
        "epochs = 20\r\n",
        "optimizer = 'adam'\r\n",
        "loss = 'categorical_crossentropy'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTPHWpsXC5lS"
      },
      "source": [
        "### Generator definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vlbyk8FC5lS"
      },
      "source": [
        "def create_flow(datagen):\r\n",
        "  return datagen.flow_from_directory(\r\n",
        "    directory = train_data_directory,\r\n",
        "    target_size = size,\r\n",
        "    color_mode = 'rgb',\r\n",
        "    class_mode = 'categorical',\r\n",
        "    batch_size = batch_size,\r\n",
        "    shuffle = True\r\n",
        "  )\r\n",
        "\r\n",
        "train_flow = create_flow(train_datagen)\r\n",
        "validation_flow = create_flow(test_datagen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o16EQ-3C5lS"
      },
      "source": [
        "### Model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tydto8ruC5lS"
      },
      "source": [
        "model = tf.keras.applications.MobileNetV2(\r\n",
        "  input_shape = (*size, 3),\r\n",
        "  alpha = alpha,\r\n",
        "  include_top = True,\r\n",
        "  pooling = pooling,\r\n",
        "  weights = weights,\r\n",
        "  classes = train_flow.num_classes,\r\n",
        "  classifier_activation = activation\r\n",
        ")\r\n",
        "\r\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\r\n",
        "#tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcr-qVSwC5lS"
      },
      "source": [
        "### Training phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4YYQf_JC5lS"
      },
      "source": [
        "history = model.fit(\r\n",
        "  train_flow,\r\n",
        "  validation_data = validation_flow,\r\n",
        "  validation_steps = validation_flow.samples // batch_size,\r\n",
        "  epochs = epochs,\r\n",
        "  steps_per_epoch = train_flow.samples // batch_size\r\n",
        ")\r\n",
        "\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('Model accuracy')\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skrznC1HC5lS"
      },
      "source": [
        "## Store images visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J92juWbnC5lT"
      },
      "source": [
        "def show_shelf_with_bbox_and_classes(indexes, columns, dataset):\r\n",
        "  def show_single_shelf(row, sp):\r\n",
        "    color = (255, 255, 0)\r\n",
        "    img = row.image.copy()\r\n",
        "    for item in row.items:\r\n",
        "      bbox, label, class_index  = item\r\n",
        "      [xmin, xmax, ymin, ymax] = bbox\r\n",
        "      height, width, _ = img.shape\r\n",
        "      xmin = int(xmin * width)\r\n",
        "      xmax = int(xmax * width)\r\n",
        "      ymin = int(ymin * height)\r\n",
        "      ymax = int(ymax * height)\r\n",
        "      thickness = round(max(width, height) * 0.01)\r\n",
        "      cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color, thickness)\r\n",
        "      cv2.putText(img,\r\n",
        "                  class_name(class_index),\r\n",
        "                  (round(xmin + thickness * 1.5), round(ymin + thickness * 3)),\r\n",
        "                  cv2.FONT_HERSHEY_SIMPLEX,\r\n",
        "                  2.5,\r\n",
        "                  color,\r\n",
        "                  thickness=round(thickness/2))\r\n",
        "    plt.axis('off')\r\n",
        "    plt.imshow(img)\r\n",
        "  dataset_plot_grid(indexes, columns, dataset, show_single_shelf)\r\n",
        "\r\n",
        "show_shelf_with_bbox_and_classes(np.random.randint(0, len(store), 4), 2, store)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
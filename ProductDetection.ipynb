{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProductDetection.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ldeluigi/supermarket-2077-product-vision/blob/master/ProductDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVug6f_Gtc5S"
      },
      "source": [
        "# Preliminary Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdlCQFyABGy3"
      },
      "source": [
        "## Download datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQmWhRKUE5oy"
      },
      "source": [
        "!rm -rf sample_data\n",
        "!gdown --id 1fDr4g4wbnSRkuCYyS3wpuJS7Ax22bVB_ -O all.zip\n",
        "!unzip -oq all.zip\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fG1BAIjUP_7"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLso_ssrUTuB"
      },
      "source": [
        "import scipy.io\n",
        "import os\n",
        "from pathlib import Path\n",
        "import re\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import itertools\n",
        "import shutil\n",
        "from tqdm.notebook import tqdm\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k8wnPIHwMR4"
      },
      "source": [
        "# Data Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcB1_JdoNOBu"
      },
      "source": [
        "## Data visualization utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x36OMLMbXC4"
      },
      "source": [
        "def show_image(img):\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)\n",
        "\n",
        "def show_grayscale_image(img):\n",
        "  show_image(cv2.merge([img, img, img]))\n",
        "\n",
        "def plot_grid(images, columns, show_axis=False, labels=None):\n",
        "  if len(images) == 0 or columns <= 0:\n",
        "    return\n",
        "  height = 1 + math.ceil(len(images) / columns) * 2\n",
        "  width = columns * 4\n",
        "  dpi = max(images[0].shape[0], images[0].shape[1]) // 2\n",
        "  fig = plt.figure(figsize=(width, height), dpi=dpi)\n",
        "  fig.subplots_adjust(hspace=0.4)\n",
        "  for index, img in enumerate(images, start=1):\n",
        "    if 'float' in img.dtype.str:\n",
        "      img = (img * 255).astype('uint8')\n",
        "    sp = fig.add_subplot(math.ceil(len(images) / columns), columns, index)\n",
        "    if not show_axis:\n",
        "      plt.axis('off')\n",
        "    plt.imshow(img)\n",
        "    if labels is not None:\n",
        "      l = len(labels)\n",
        "      sp.set_title(labels[(index-1) % l], fontsize=10)\n",
        "    else:\n",
        "      sp.set_title(index, fontsize=10)\n",
        "\n",
        "def dataset_plot_grid(indexes, columns, dataset, draw_item):\n",
        "  fig = plt.figure(figsize=(12, 6), dpi=120)\n",
        "  # fig.subplots_adjust(hspace=0.2)\n",
        "  for index, i_img in enumerate(indexes, start=1):\n",
        "    sp = fig.add_subplot(math.ceil(len(indexes) / columns), columns, index)\n",
        "    row = dataset[i_img]\n",
        "    draw_item(row, sp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IifnByBewpq9"
      },
      "source": [
        "# Raw image preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c-J_rx58YtO"
      },
      "source": [
        "## Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDIdYTwp8ep0"
      },
      "source": [
        "training_dirname = 'Training'\n",
        "\n",
        "def create_class_label(class_index, class_name):\n",
        "  return class_name\n",
        "\n",
        "def read_classes():\n",
        "  mat = scipy.io.loadmat(os.path.join(training_dirname, 'TrainingClassesIndex.mat'))\n",
        "  raw_classes = list(map(lambda x: x[0], mat['classes'][0]))\n",
        "  classes = map(lambda x: (x[0], create_class_label(*x)), enumerate(raw_classes, start=1))\n",
        "  return dict(classes), dict(enumerate(raw_classes, start=1))\n",
        "\n",
        "def read_training_data(classes):\n",
        "  result = []\n",
        "  for class_index, class_name in classes.items():\n",
        "    dirname_images = os.path.join(training_dirname, class_name)\n",
        "    directory_images = os.fsencode(dirname_images)\n",
        "    for file in os.listdir(directory_images):\n",
        "      img = cv2.imread(os.path.join(dirname_images, os.fsdecode(file)))\n",
        "      img_rgb =  cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "      result.append((img_rgb, class_index))\n",
        "  return np.rec.array(result, dtype=[('image', 'O'), ('class_index', 'i4')])\n",
        "\n",
        "def read_store_data(storename):\n",
        "  dirname_anno = os.path.join(storename, 'annotation')\n",
        "  dirname_images = os.path.join(storename, 'images')\n",
        "  directory_anno = os.fsencode(dirname_anno)\n",
        "  directory_images = os.fsencode(dirname_images)\n",
        "\n",
        "  result = []\n",
        "\n",
        "  for file in os.listdir(directory_anno):\n",
        "    filename = os.fsdecode(file)\n",
        "    if filename.endswith(\".mat\"): \n",
        "      mat = scipy.io.loadmat(os.path.join(dirname_anno, filename))\n",
        "      number = re.search(r'^anno.(\\d+).mat$', filename).group(1)\n",
        "      img = cv2.imread(os.path.join(dirname_images, number + '.jpg'))\n",
        "\n",
        "      img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "      img_annotation = mat['annotation'][0, 0]\n",
        "      bboxes = map(lambda x: x[0], img_annotation[0][0])\n",
        "      labels = map(lambda x: str(x[0][0][0]), img_annotation[1][0])\n",
        "      class_indexes = img_annotation[2][0]\n",
        "      result.append((img_rgb, list(zip(bboxes, labels, class_indexes))))\n",
        "  return np.rec.array(result, dtype=[('image', 'O'), ('items', 'O')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUSUqmzUNoST"
      },
      "source": [
        "## Prepare products class dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exz2n1l6NoSW"
      },
      "source": [
        "classes, raw_classes = read_classes()\n",
        "\n",
        "def class_name(class_index):\n",
        "  return classes[class_index] if class_index >= 0 else None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3MOBrBrNoSW"
      },
      "source": [
        "## Load training raw images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_yK63lINoSW"
      },
      "source": [
        "products = read_training_data(raw_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdgzi0lC1x3T"
      },
      "source": [
        "## Products visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcLzx8uY10oS"
      },
      "source": [
        "def show_products_with_class(indexes, columns, dataset):\n",
        "  def show_single_product_with_class(row, sp):\n",
        "    plt.axis('off')\n",
        "    plt.imshow(row.image)\n",
        "    sp.set_title(class_name(row.class_index), fontsize=10)\n",
        "  dataset_plot_grid(indexes, columns, dataset, show_single_product_with_class)\n",
        "\n",
        "show_products_with_class(np.random.randint(0, len(products), 6), 3, products)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czcedvVCNoSX"
      },
      "source": [
        "## Image preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM9BsdVmNoSX"
      },
      "source": [
        "### Background removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go6lv7htNoSX"
      },
      "source": [
        "# code taken from https://www.kaggle.com/vadbeg/opencv-background-removal and modified\n",
        "\n",
        "def remove_background(img, threshold):\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    _, threshed = cv2.threshold(gray, threshold, 255, cv2.THRESH_BINARY_INV)\n",
        "\n",
        "    kernel_size = round(max(img.shape[0], img.shape[1]) * 0.02)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n",
        "    morphed = cv2.morphologyEx(threshed, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "    cnts = cv2.findContours(morphed, \n",
        "                            cv2.RETR_EXTERNAL,\n",
        "                            cv2.CHAIN_APPROX_SIMPLE)[0] # should be [1] for cv2 version <= 4\n",
        "\n",
        "    cnts = sorted(cnts, key=cv2.contourArea)\n",
        "\n",
        "    mask = cv2.drawContours(threshed, [cnts[-1]], 0, [255], cv2.FILLED)\n",
        "    masked_data = cv2.bitwise_and(img, img, mask=mask)\n",
        "\n",
        "    x, y, w, h = cv2.boundingRect(cnts[-1])\n",
        "    dst = masked_data[y: y + h, x: x + w]\n",
        "\n",
        "    alpha = mask[y: y + h, x: x + w]\n",
        "    r, g, b = cv2.split(dst)\n",
        "\n",
        "    rgba = [r, g, b, alpha]\n",
        "    dst = cv2.merge(rgba, 4)\n",
        "    return dst\n",
        "\n",
        "n = np.random.randint(products.shape[0])\n",
        "print(f'Index: {n}')\n",
        "print(f'Class: {class_name(products[n].class_index)}')\n",
        "plot_grid([products[n].image, remove_background(products[n].image, 250)], 2, show_axis=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7z17jtMNoSX"
      },
      "source": [
        "### Image resize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICMAZEihNoSY"
      },
      "source": [
        "def resize_image(img, size, color=[0,0,0,0]):\n",
        "  target_w, target_h = size\n",
        "  original_h, original_w, _ = img.shape\n",
        "  target_ar = target_w / target_h\n",
        "  original_ar = original_w / original_h\n",
        "\n",
        "  scale_factor = target_h / original_h if target_ar > original_ar else target_w / original_w\n",
        "  scaled_w = round(original_w * scale_factor)\n",
        "  scaled_h = round(original_h * scale_factor)\n",
        "  scaled_size = (scaled_w, scaled_h)\n",
        "  resized = cv2.resize(img, scaled_size)\n",
        "\n",
        "  delta_h = target_h - scaled_h\n",
        "  delta_w = target_w - scaled_w\n",
        "  top    = delta_h // 2\n",
        "  left   = delta_w // 2\n",
        "  bottom = delta_h - top\n",
        "  right  = delta_w - left\n",
        "\n",
        "  return cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)\n",
        "\n",
        "n = np.random.randint(products.shape[0])\n",
        "image = products[n].image\n",
        "image = remove_background(image, 250)\n",
        "plot_grid([image, resize_image(image, (400, 400))], 2, show_axis=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LR8gLNUNoSY"
      },
      "source": [
        "## Dataset preparation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOyo96qdNoSY"
      },
      "source": [
        "### Image cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErK6wz2TNoSY"
      },
      "source": [
        "size = (224, 224)\n",
        "\n",
        "def clean_image(img):\n",
        "  threshold = 250\n",
        "  img = remove_background(img, threshold)\n",
        "  return resize_image(img, size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q7xa0oHNoSY"
      },
      "source": [
        "### Export new dataset on disk\n",
        "\n",
        "Optionally reduce it to save memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tec-feRdNoSZ"
      },
      "source": [
        "number_of_images_to_save = 1000\n",
        "reduce_dataset = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkmiotkwNoSZ"
      },
      "source": [
        "train_data_directory = 'Temp'\n",
        "shutil.rmtree(train_data_directory, ignore_errors=True)\n",
        "\n",
        "index_to_class_map = dict()\n",
        "def index_to_class(index):\n",
        "  # We could search in products for O(n)\n",
        "  # Instead we use a map for O(1)\n",
        "  return index_to_class_map[index]\n",
        "\n",
        "if reduce_dataset:\n",
        "  products_size = number_of_images_to_save\n",
        "  products_to_dump = np.random.choice(products.shape[0], number_of_images_to_save, replace = False)\n",
        "else:\n",
        "  products_size = products.shape[0]\n",
        "  products_to_dump = np.arange(products_size)\n",
        "\n",
        "for index, product_index in tqdm(enumerate(products_to_dump), total=products_size, desc='Writing files...'):\n",
        "  image, class_index = products[product_index]\n",
        "  index_str = f'{index:04d}'\n",
        "  output_dir = os.path.join(train_data_directory, index_str)\n",
        "  Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "  out = cv2.cvtColor(clean_image(image), cv2.COLOR_RGBA2BGRA)\n",
        "  cv2.imwrite(os.path.join(output_dir, f'{index_str}.png'), out)\n",
        "  index_to_class_map[index] = class_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAFVv8JIxoKK"
      },
      "source": [
        "# Computer vision model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WQgfV_JyV10"
      },
      "source": [
        "## High-level algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsDVZMnqjiXT"
      },
      "source": [
        "def accuracy(extract_features, compute_feature_distance, threshold, n = 100):\n",
        "  actual = []\n",
        "  predicted = []\n",
        "  false_positives = []\n",
        "  false_negatives = []\n",
        "  it = probability_merge(mismatched_images(), matched_images(), p = 0.5)\n",
        "  for _ in tqdm(range(n), total=n, desc='Calculating accuracy...'):\n",
        "    original, transformed, label = next(it)\n",
        "    original_features = extract_features(original)\n",
        "    transformed_features = extract_features(transformed)\n",
        "    distance = compute_feature_distance(original_features, transformed_features)\n",
        "    prediction = 1 if distance > threshold else 0\n",
        "    actual.append(label)\n",
        "    predicted.append(prediction)\n",
        "    if label != prediction:\n",
        "      (false_positives if prediction == 0 else false_negatives).append((original, transformed))\n",
        "  confusion = confusion_matrix(actual, predicted)\n",
        "  print(confusion)\n",
        "  to_be_plotted = []\n",
        "  for original, transformed in false_positives:\n",
        "    to_be_plotted.append(original)\n",
        "    to_be_plotted.append(transformed)\n",
        "  plot_grid(to_be_plotted, 2, labels=['Original', 'False positive'])\n",
        "\n",
        "  to_be_plotted = []\n",
        "  for original, transformed in false_negatives:\n",
        "    to_be_plotted.append(original)\n",
        "    to_be_plotted.append(transformed)\n",
        "  plot_grid(to_be_plotted, 2, labels=['Original', 'False negative'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU2HGhCezFRH"
      },
      "source": [
        "feature_cache = None\n",
        "feature_cache_name = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiaWITTaEa0f"
      },
      "source": [
        "def cache_features(extract_features):\n",
        "  global feature_cache, feature_cache_name\n",
        "  if extract_features.__name__ != feature_cache_name:\n",
        "    feature_cache_name = None\n",
        "  if feature_cache is None or feature_cache_name is None:\n",
        "    feature_cache_name = extract_features.__name__\n",
        "    feature_cache = []\n",
        "    for image, index in tqdm(single_images(single_pass = True), desc = 'Creating feature cache...'):\n",
        "      features = extract_features(image)\n",
        "      feature_cache.append((features, index))\n",
        "  return feature_cache\n",
        "\n",
        "def product_classification(img, extract_features, compute_feature_distance):\n",
        "  img_features = extract_features(img)\n",
        "  feature_db = cache_features(extract_features)\n",
        "  distances_iter = map(lambda fc: compute_feature_distance(fc[0], img_features), feature_db)\n",
        "  index, best_distance = min(enumerate(distances_iter), key=lambda x:x[1])\n",
        "  best_match_label = feature_db[index][1]\n",
        "  best_match_index = int(best_match_label)\n",
        "  class_index = index_to_class(best_match_index)\n",
        "  return best_match_index, class_index, class_name(class_index), best_distance\n",
        "\n",
        "def product_classifications(imgs, extract_features, compute_feature_distance):\n",
        "  return list(map(lambda im: product_classification(im, extract_features, compute_feature_distance), imgs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y57LoZkRNoSZ"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRy8VWHfNoSZ"
      },
      "source": [
        "### 3D rotation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91PXTIneNoSZ"
      },
      "source": [
        "def image_3D_rotation(img, theta = 0, phi = 0, gamma = 0, dx = 0, dy = 0, dz = 0):\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "      img       : the image data as numpy array\n",
        "      theta     : rotation around the x axis\n",
        "      phi       : rotation around the y axis\n",
        "      gamma     : rotation around the z axis (basically a 2D rotation)\n",
        "      dx        : translation along the x axis\n",
        "      dy        : translation along the y axis\n",
        "      dz        : translation along the z axis (distance to the image)\n",
        "  Output:\n",
        "      image     : the rotated image\n",
        "  \n",
        "  Reference:\n",
        "      1.        : http://stackoverflow.com/questions/17087446/how-to-calculate-perspective-transform-for-opencv-from-rotation-angles\n",
        "      2.        : http://jepsonsblog.blogspot.tw/2012/11/rotation-in-3d-using-opencvs.html\n",
        "      3.        : Code taken from https://github.com/eborboihuc/rotate_3d/blob/master/image_transformer.py\n",
        "  \"\"\"\n",
        "  def deg_to_rad(deg):\n",
        "    return deg * math.pi / 180.0\n",
        "  def get_M(theta, phi, gamma, dx, dy, dz, size, focal):\n",
        "    w = size[0]\n",
        "    h = size[1]\n",
        "    f = focal\n",
        "    # Projection 2D -> 3D matrix\n",
        "    A1 = np.array([ [1, 0, -w/2],\n",
        "                    [0, 1, -h/2],\n",
        "                    [0, 0, 1],\n",
        "                    [0, 0, 1]])\n",
        "    # Rotation matrices around the X, Y, and Z axis\n",
        "    RX = np.array([ [1, 0, 0, 0],\n",
        "                    [0, np.cos(theta), -np.sin(theta), 0],\n",
        "                    [0, np.sin(theta), np.cos(theta), 0],\n",
        "                    [0, 0, 0, 1]])\n",
        "    RY = np.array([ [np.cos(phi), 0, -np.sin(phi), 0],\n",
        "                    [0, 1, 0, 0],\n",
        "                    [np.sin(phi), 0, np.cos(phi), 0],\n",
        "                    [0, 0, 0, 1]])\n",
        "    RZ = np.array([ [np.cos(gamma), -np.sin(gamma), 0, 0],\n",
        "                    [np.sin(gamma), np.cos(gamma), 0, 0],\n",
        "                    [0, 0, 1, 0],\n",
        "                    [0, 0, 0, 1]])\n",
        "    # Composed rotation matrix with (RX, RY, RZ)\n",
        "    R = np.dot(np.dot(RX, RY), RZ)\n",
        "    # Translation matrix\n",
        "    T = np.array([  [1, 0, 0, dx],\n",
        "                    [0, 1, 0, dy],\n",
        "                    [0, 0, 1, dz],\n",
        "                    [0, 0, 0, 1]])\n",
        "    # Projection 3D -> 2D matrix\n",
        "    A2 = np.array([ [f, 0, w/2, 0],\n",
        "                    [0, f, h/2, 0],\n",
        "                    [0, 0, 1, 0]])\n",
        "    # Final transformation matrix\n",
        "    return np.dot(A2, np.dot(T, np.dot(R, A1)))\n",
        "  height = img.shape[0]\n",
        "  width = img.shape[1]\n",
        "  num_channels = img.shape[2]\n",
        "  rtheta = deg_to_rad(theta)\n",
        "  rphi = deg_to_rad(phi)\n",
        "  rgamma = deg_to_rad(gamma)\n",
        "  d = np.sqrt(height**2 + width**2)\n",
        "  focal = d / (2 * np.sin(rgamma) if np.sin(rgamma) != 0 else 1)\n",
        "  dz = focal\n",
        "  mat = get_M(rtheta, rphi, rgamma, dx, dy, dz, (width, height), focal)\n",
        "  return cv2.warpPerspective(img.copy(), mat, (width, height))\n",
        "\n",
        "def random_spatial_rotation(theta_range, phi_range, gamma_range):\n",
        "  return lambda img: image_3D_rotation(\n",
        "    img, \n",
        "    theta = np.random.randint(theta_range[0], theta_range[1] + 1),\n",
        "    phi = np.random.randint(phi_range[0], phi_range[1] + 1),\n",
        "    gamma = np.random.randint(gamma_range[0], gamma_range[1] + 1)\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neTkXKe3NoSa"
      },
      "source": [
        "### Data generator parameters definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlxioavxNoSa"
      },
      "source": [
        "real_datagen = ImageDataGenerator(\n",
        "    data_format = 'channels_last',\n",
        ")\n",
        "\n",
        "augmented_datagen = ImageDataGenerator(\n",
        "    brightness_range = [0.5, 1.2],\n",
        "    width_shift_range = size[0] // 10,\n",
        "    height_shift_range = size[1] // 10,\n",
        "    zoom_range = 0.1,\n",
        "    fill_mode = 'constant',\n",
        "    cval = 0,\n",
        "    data_format = 'channels_last',\n",
        "    preprocessing_function = random_spatial_rotation(\n",
        "      theta_range = (-20, 20),\n",
        "      phi_range = (-30, 30),\n",
        "      gamma_range = (-10, 10)\n",
        "    )\n",
        ")\n",
        "\n",
        "def create_flow(single_pass = False):\n",
        "  rd = real_datagen.flow_from_directory(\n",
        "    directory = train_data_directory,\n",
        "    target_size = size,\n",
        "    color_mode = 'rgba',\n",
        "    class_mode = 'sparse',\n",
        "    batch_size = 1,\n",
        "    shuffle = True\n",
        "  )\n",
        "  if single_pass:\n",
        "    return itertools.islice(rd, rd.samples)\n",
        "  return rd\n",
        "\n",
        "def flow_to_augmented_tuple(t):\n",
        "  rescale = 1./255\n",
        "  original = t[0][0] * rescale\n",
        "  transformed = augmented_datagen.random_transform(augmented_datagen.preprocessing_function(t[0][0])) * rescale\n",
        "  label = t[1][0]\n",
        "  return original, transformed, label\n",
        "\n",
        "def flow_to_tuple(t):\n",
        "  rescale = 1./255\n",
        "  original = t[0][0] * rescale\n",
        "  label = t[1][0]\n",
        "  return original, label\n",
        "\n",
        "def mismatched_images():\n",
        "  flow_1 = create_flow()\n",
        "  flow_2 = create_flow()\n",
        "  while True:\n",
        "    t_1 = next(flow_1)\n",
        "    t_2 = next(flow_2)\n",
        "    if t_1[1][0] != t_2[1][0]:\n",
        "      image, _ = flow_to_tuple(t_1)\n",
        "      _, transformed, _ = flow_to_augmented_tuple(t_2)\n",
        "      yield image, transformed\n",
        "    else:\n",
        "      next(flow_1) # used for misaligning iterators\n",
        "\n",
        "def matched_images(single_pass = False):\n",
        "  flow = create_flow(single_pass)\n",
        "  for original, transformed, _ in map(flow_to_augmented_tuple, flow):\n",
        "    yield original, transformed\n",
        "\n",
        "def single_images(single_pass = False):\n",
        "  flow = create_flow(single_pass)\n",
        "  return map(flow_to_tuple, flow)\n",
        "\n",
        "def single_altered_images(single_pass = False):\n",
        "  flow = create_flow(single_pass)\n",
        "  def tuple_f(t):\n",
        "    _, transformed, label = flow_to_augmented_tuple(t)\n",
        "    return transformed, label\n",
        "  return map(tuple_f, flow)\n",
        "\n",
        "res = []\n",
        "it = matched_images()\n",
        "for _ in range(10):\n",
        "  t = next(it)\n",
        "  res.append(t[0])\n",
        "  res.append(t[1])\n",
        "\n",
        "plot_grid(res, 2, labels=[\"Original\", \"Same, altered\"])\n",
        "\n",
        "res = []\n",
        "it = mismatched_images()\n",
        "for _ in range(10):\n",
        "  t = next(it)\n",
        "  res.append(t[0])\n",
        "  res.append(t[1])\n",
        "\n",
        "plot_grid(res, 2, labels=[\"Original\", \"Other, altered\"])\n",
        "del res, it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO85Ns5VPXEH"
      },
      "source": [
        "def probability_merge(it_1, it_2, p = 0.5):\n",
        "  while True:\n",
        "    rand = np.random.random()\n",
        "    it, label = (it_1, 1) if rand < p else (it_2, 0)\n",
        "    original, transformed = next(it)\n",
        "    yield original, transformed, label\n",
        "\n",
        "it = probability_merge(mismatched_images(), matched_images(), p = 0.5)\n",
        "res = []\n",
        "for _ in range(20):\n",
        "  t = next(it)\n",
        "  res.append(t[0])\n",
        "  res.append(t[1])\n",
        "\n",
        "plot_grid(res, 2, labels=[\"First\", \"Second\"])\n",
        "del res, it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve7k3hkRyqof"
      },
      "source": [
        "## Model definition\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0RuRDhs_km-"
      },
      "source": [
        "### Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_DnJqjT_knE"
      },
      "source": [
        "def create_bf_matcher():\n",
        "  return cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
        "\n",
        "def create_flann_matcher():\n",
        "  FLANN_INDEX_LSH = 6\n",
        "  index_params = dict(\n",
        "    algorithm = FLANN_INDEX_LSH,\n",
        "    table_number = 6,     # 12\n",
        "    key_size = 12,        # 20\n",
        "    multi_probe_level = 1 # 2\n",
        "  )\n",
        "  search_params = dict(checks = 50)\n",
        "  return cv2.FlannBasedMatcher(index_params, search_params)\n",
        "\n",
        "def get_channel_matches(p, t):\n",
        "  k_1, d_1 = p\n",
        "  k_2, d_2 = t\n",
        "  if d_1 is None or d_2 is None:\n",
        "    return []\n",
        "  #matcher = create_bf_matcher()\n",
        "  matcher = create_flann_matcher()\n",
        "  return matcher.match(d_1, d_2)\n",
        "\n",
        "def compare_features(predictions, targets):\n",
        "  distances = []\n",
        "  for p, t in zip(predictions, targets):\n",
        "    matches = get_channel_matches(p, t)\n",
        "    n_matches = len(matches)\n",
        "    if n_matches == 0:\n",
        "      distance = 1e19\n",
        "    else:\n",
        "      distance = sum(filter(lambda d: True, map(lambda m: m.distance, matches))) / n_matches\n",
        "    distances.append(distance)\n",
        "      # 1 / sum(map(lambda m: 1 / (m.distance ** 2 + epsilon), matches))\n",
        "\n",
        "def create_feature_extractor():\n",
        "  orb = cv2.ORB_create(edgeThreshold=25, nfeatures=500) # https://docs.opencv.org/master/d1/d89/tutorial_py_orb.html\n",
        "  return orb\n",
        "\n",
        "feature_extractor = create_feature_extractor()\n",
        "\n",
        "def extract_features_raw(img):\n",
        "  image_hsv = cv2.cvtColor(img * 255, cv2.COLOR_RGB2HSV).astype('uint8')\n",
        "  h, s, v = cv2.split(image_hsv)\n",
        "  h_f = feature_extractor.detectAndCompute(h, None)\n",
        "  s_f = feature_extractor.detectAndCompute(s, None)\n",
        "  v_f = feature_extractor.detectAndCompute(v, None)\n",
        "  return h_f, s_f, v_f\n",
        "\n",
        "def extract_features(img, blur_size = 5):\n",
        "  blurred_img = cv2.GaussianBlur(img, (blur_size, blur_size), 0) if blur_size > 1 else img\n",
        "  return extract_features_raw(blurred_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEZtPiNvbYoB"
      },
      "source": [
        "def show_image_comparison(first_img, second_img):\n",
        "  first_img_gray = cv2.cvtColor(first_img * 255, cv2.COLOR_RGB2GRAY).astype('uint8')\n",
        "  second_img_gray = cv2.cvtColor(second_img * 255, cv2.COLOR_RGB2GRAY).astype('uint8')\n",
        "  first_k, first_d = extract_features(first_img)[0]\n",
        "  second_k, second_d = extract_features(second_img)[0]\n",
        "  first_sift_image = cv2.drawKeypoints(first_img_gray, first_k, first_img)\n",
        "  second_sift_image = cv2.drawKeypoints(second_img_gray, second_k, second_img)\n",
        "  matcher = create_flann_matcher()\n",
        "  matches = matcher.match(first_d, second_d)\n",
        "  matches = sorted(matches, key = lambda x:x.distance)\n",
        "  matched_img = cv2.drawMatches(first_img_gray, first_k, first_img, second_k, matches[:70], second_img, flags=2)\n",
        "  plot_grid([first_sift_image, second_sift_image, matched_img], 2, labels=['Original ORB', 'Altered ORB', 'Matches'])\n",
        "\n",
        "show_image_comparison(*next(matched_images()))\n",
        "show_image_comparison(*next(mismatched_images()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIXcUiuSTpXz"
      },
      "source": [
        "def stats(it, n = 1):\n",
        "  results = []\n",
        "  for _ in range(n):\n",
        "    original, transformed = next(it)\n",
        "    original_f = extract_features(original)\n",
        "    transformed_f = extract_features(transformed)\n",
        "    if original_f[1] is None or transformed_f[1] is None:\n",
        "      continue\n",
        "    results.append(compare_features(original_f, transformed_f))\n",
        "  print('\\tAvg:', np.mean(results))\n",
        "  print('\\tMedian:', np.median(results))\n",
        "  print('\\tMax:', np.max(results))\n",
        "  print('\\tMin:', np.min(results))\n",
        "  print('\\tStdev:', np.std(results))\n",
        "\n",
        "print(\"Summary of matched images feature comparison:\")\n",
        "stats(matched_images(), 500)\n",
        "print(\"Summary of mismatched images feature comparison:\")\n",
        "stats(mismatched_images(), 500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvwB90dcSdL7"
      },
      "source": [
        "### Feature classification\n",
        "Instead of the clusterization phase you can find in literature, we use SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5i2c56WSw0F"
      },
      "source": [
        "\n",
        "\n",
        "distance_threshold = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvYz3NY1E3Rt"
      },
      "source": [
        "### Evaluate performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGwsPYm-E3Rt"
      },
      "source": [
        "\n",
        "accuracy(extract_features, compare_features, distance_threshold, n = 400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FN2GLT8E3Rt"
      },
      "source": [
        "def column(M, c):\n",
        "  return [row[c] for row in M]\n",
        "test_size = 100\n",
        "test_images = list(itertools.islice(single_altered_images(), test_size))\n",
        "predictions = product_classifications(column(test_images, 0), extract_features, compare_features)\n",
        "cm = confusion_matrix(column(test_images, 1), column(predictions, 0))\n",
        "print(\"Accuracy: \", np.trace(cm) / test_size)\n",
        "plot_grid(column(test_images[:20], 0), 5, labels=column(predictions[:20], 2))\n",
        "del test_images, predictions, cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNhvyoBjpz3t"
      },
      "source": [
        "# Testing model on stores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtjIpcNK7ap1"
      },
      "source": [
        "store = read_store_data('store2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aQMlMih6RZU"
      },
      "source": [
        "img, items = store[1]\n",
        "\n",
        "image_portion = img\n",
        "resized_portion = resize_image(image_portion, size)\n",
        "show_image(img)\n",
        "res = product_classification(resized_portion, extract_features, compute_feature_distance = rmse)\n",
        "print(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpsgqbT7Gd-W"
      },
      "source": [
        "def sliding_window(img, size, stride=(0.5, 0.5)):\n",
        "  scale_values=[0.5, 0.3]\n",
        "  height, width, _ = img.shape\n",
        "  aspect_ratio = size[1] / size[0]\n",
        "  for scale in scale_values:\n",
        "    scaled_width = int(width * scale)\n",
        "    scaled_height = int(height * scale)\n",
        "    scaled_img = resize_image(img, (scaled_width, scaled_height))\n",
        "    y_min = 0\n",
        "    while y_min + size[1] < scaled_height:\n",
        "      x_min = 0\n",
        "      while x_min + size[0] < scaled_width:\n",
        "        yield scaled_img[y_min : y_min + size[0] - 1, x_min : x_min + size[1] - 1]\n",
        "        x_min += int(size[0] * stride[0])\n",
        "      y_min += int(size[1] * stride[1])\n",
        "\n",
        "res = list(sliding_window(img, size))\n",
        "\n",
        "plot_grid(res, 9)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}